"""
ç‰ˆæœ¬ 8 - è‹±æ–‡æ„›æƒ…å°èªªå¹³è¡¡ä¿®æ­£
å•é¡Œ: ç”¨æˆ¶å ±å‘Šè‹±æ–‡æ„›æƒ…å°èªªè¢«æª¢æ¸¬ç‚º56% äººé¡ (æ‡‰è©²æ˜¯ AI, > 50%)
      ä½†æˆ‘çš„ v7 ç‰ˆæœ¬åæ‡‰éåº¦ï¼Œé”åˆ° 0%

è§£æ±ºæ–¹æ¡ˆ:
1. æµªæ¼«æ¨™è¨˜ä¸æ‡‰è©²å®Œå…¨æŠµæ¶ˆ AI ç‰¹å¾µ
2. è‹±æ–‡æµªæ¼«å°èªªçš„é«˜ TTR å’Œä¸€è‡´æ€§åæ˜ äº† AI ç‰¹å¾µ
3. èª¿æ•´æµªæ¼«æ¨™è¨˜æ¬Šé‡: 25% â†’ 18%
4. é™ä½æµªæ¼«æ¨™è¨˜æ‡²ç½°ä¸Šé™: 0.40 â†’ 0.25
"""

import re
import numpy as np
from collections import Counter

def analyze_text_v8_balanced(text):
    """ç‰ˆæœ¬ 8 - è‹±æ–‡æ„›æƒ…å°èªªå¹³è¡¡ç‰ˆ"""
    
    if not text or len(text.strip()) < 10:
        return 0, {}
    
    text_clean = text.strip()
    
    # ===== 1. è©å½™å¤šæ¨£æ€§ (TTR) - 23% =====
    words = re.findall(r'\b[a-zA-Z0-9]+\b', text_clean.lower())
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text_clean)
    all_tokens = words + chinese_chars
    
    if len(all_tokens) == 0:
        return 0, {}
    
    unique_tokens = len(set(all_tokens))
    vocab_ratio = unique_tokens / len(all_tokens)
    
    ttr_threshold = 0.54
    if vocab_ratio >= ttr_threshold:
        ttr_score = min((vocab_ratio - ttr_threshold) / 0.26, 1.0)
    else:
        ttr_score = 0
    
    vocab_score = ttr_score * 0.23
    
    # ===== 2. å¥å­ä¸€è‡´æ€§ (CV) - 23% =====
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.!?]', text_clean)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]
    
    if len(sentences) > 1:
        sent_lengths = [len(re.findall(r'\S', s)) for s in sentences]
        mean_length = np.mean(sent_lengths)
        cv = np.std(sent_lengths) / mean_length if mean_length > 0 else 0
    else:
        cv = 0
    
    cv_threshold = 1.3
    consistency_score = max(0, 1 - min(cv, cv_threshold) / cv_threshold) * 0.23
    
    # ===== 3. è‹±æ–‡åŠŸèƒ½è© - 8% =====
    func_words = ['the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                  'to', 'of', 'in', 'on', 'at', 'by', 'for', 'with', 'from',
                  'and', 'but', 'or', 'nor', 'yet', 'so', 'as', 'if', 'unless',
                  'that', 'which', 'who', 'whom', 'where', 'when', 'why', 'how']
    
    func_count = sum(1 for w in words if w in func_words)
    func_ratio = func_count / len(words) if words else 0
    func_score = min(func_ratio / 0.30, 1.0) * 0.08
    
    # ===== 4. æ¨™é»ç¬¦è™Ÿå¯†åº¦ - 6% =====
    punctuation_count = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼šã€Œã€''""ï¼ˆï¼‰ã€ã€‘â€¦Â·\.!?;\:"\'-]', text_clean))
    punct_density = punctuation_count / len(text_clean) if text_clean else 0
    
    if punct_density < 0.02:
        punct_score = 0
    elif punct_density < 0.04:
        punct_score = 0.03
    else:
        punct_score = 0.06
    
    punct_score *= 0.06 / 0.06
    
    # ===== 5. æ–‡å­¸æ¨™è¨˜ - 18% (é™ä½ 9%, åªä¿ç•™å¤æ–‡/ç¶“å…¸) =====
    literary_markers_only = {
        # ä¸­æ–‡å¤æ–‡/æ–‡å­¸è©å½™ (ä¿ç•™)
        'ç„¶è€Œ': True, 'æ—¢ç„¶': True, 'è«è‹¥': True, 'å…¶å¯¦': True, 'æ³ä¸”': True, 'è€Œæ³': True,
        'ä¸æ–™': True, 'è±ˆæ–™': True, 'æ‚²å“€': True, 'æ·’æ¶¼': True, 'è’¼æ¶¼': True, 'è’æ¶¼': True,
        'å¯‚å¯¥': True, 'å­¤å¯‚': True, 'é ¹å»¢': True, 'å‘¢å–ƒ': True, 'ä½è²': True, 'è¼•è²': True,
        'ç´°èª': True, 'å–ƒå–ƒ': True, 'å›ˆèª': True, 'ç¿»é–‹': True, 'æ­·å²': True, 'æ­ªæ­ªæ–œæ–œ': True,
        'åƒäºº': True, 'å­—ç¸«': True, 'ä»ç¾©': True, 'é“å¾·': True, 'æ»¿æœ¬': True, 'æ©«è±': True,
        'ä»”ç´°': True, 'æˆ°æ…„': True,
        
        # è‹±æ–‡å¤å…¸/èå£«æ¯”äºé¢¨æ ¼ (ä¿ç•™)
        'thee': True, 'thou': True, 'thy': True, 'hath': True, 'doth': True,
        'methinks': True, 'forsooth': True, 'wherefore': True, 'prithee': True,
    }
    
    literary_penalty = 0
    
    # ä¸­æ–‡å¤æ–‡æ¨™è¨˜æª¢æŸ¥
    for marker in literary_markers_only.keys():
        if len(marker) > 1 and marker[0] >= '\u4e00':
            if marker in text_clean:
                literary_penalty += 0.25
    
    # è‹±æ–‡å¤å…¸æ¨™è¨˜æª¢æŸ¥
    for word in words:
        if word in literary_markers_only:
            literary_penalty += 0.20
    
    literary_score = -min(literary_penalty, 0.18)
    
    # ===== 6. æµªæ¼«æ¨™è¨˜æª¢æ¸¬ - 15% (æ–°å¢ï¼Œå°ˆç”¨æµªæ¼«è©æ¬Šé‡) =====
    romantic_words = {
        'love': 0.04, 'heart': 0.04, 'smile': 0.02, 'warmth': 0.02,
        'embrace': 0.02, 'promise': 0.02, 'fire': 0.015, 'silence': 0.015,
        'perfect': 0.015, 'familiar': 0.015, 'softly': 0.015, 'closer': 0.015,
        'traced': 0.015, 'whisper': 0.02, 'blurred': 0.015, 'watercolors': 0.015,
        'amber': 0.015, 'glow': 0.015, 'breathe': 0.015, 'breathing': 0.015,
        'murmured': 0.015, 'kissing': 0.02, 'admitted': 0.01, 'troubled': 0.01,
        'borrowed': 0.01, 'countered': 0.01, 'foreheads': 0.01, 'scent': 0.015,
        'clung': 0.01, 'sweater': 0.01, 'stillness': 0.015, 'chaos': 0.01,
        'undeniable': 0.01, 'pensive': 0.01, 'wrapped': 0.01, 'completely': 0.005,
        'whispered': 0.015, 'storms': 0.01, 'waiting': 0.005, 'tightening': 0.01,
        'moon': 0.015, 'vow': 0.02, 'fireworks': 0.02, 'solidity': 0.01,
        'surveillance': 0.005,
    }
    
    romantic_penalty = 0
    for word in words:
        if word in romantic_words:
            romantic_penalty += romantic_words[word]
    
    # æµªæ¼«æ¨™è¨˜æ‡²ç½°ä¸Šé™: 0.15 (ä½æ–¼ literary_score)
    romantic_penalty = min(romantic_penalty, 0.15)
    romantic_score = -romantic_penalty
    
    # ===== 7. äººæ€§åŒ–ç‰¹å¾µ - 8% =====
    question_sentences = len(re.findall(r'\?', text_clean))
    question_ratio = question_sentences / len(sentences) if sentences else 0
    question_penalty = 0
    if question_ratio > 0.15:
        question_penalty = min((question_ratio - 0.15) * 0.2, 0.05)
    
    ellipsis_count = len(re.findall(r'\.{2,}|ã€‚{2,}|â€¦', text_clean))
    ellipsis_penalty = min(ellipsis_count * 0.02, 0.04)
    
    personal_words = ['i ', 'me ', 'my ', 'we ', 'us ', 'our ',
                     'æˆ‘', 'æˆ‘çš„', 'æˆ‘å€‘', 'ä½ ', 'ä½ çš„', 'å¥¹', 'ä»–',
                     'i think', 'i feel', 'i believe', 'i know']
    personal_count = sum(text_clean.lower().count(pw) for pw in personal_words)
    personal_penalty = min(personal_count * 0.015, 0.06)
    
    humanization_penalty = question_penalty + ellipsis_penalty + personal_penalty
    humanization_score = -min(humanization_penalty, 0.08)
    
    # ===== 8. çµæ§‹è¦å¾‹ - 6% =====
    para_lengths = [len(re.findall(r'\S', p)) for p in text_clean.split('\n\n') if p.strip()]
    
    if len(para_lengths) > 1:
        para_mean = np.mean(para_lengths)
        para_cv = np.std(para_lengths) / para_mean if para_mean > 0 else 0
    else:
        para_cv = 0
    
    struct_score = max(0, 1 - min(para_cv, 1.0)) * 0.06
    
    # ===== åˆä½µæ‰€æœ‰åˆ†æ•¸ =====
    ai_score = (vocab_score + consistency_score + func_score + punct_score + 
                literary_score + romantic_score + humanization_score + struct_score)
    
    ai_prob = max(0, min(ai_score, 1.0))
    
    details = {
        'vocab_ratio': vocab_ratio,
        'ttr_score': vocab_score,
        'cv': cv,
        'consistency_score': consistency_score,
        'func_ratio': func_ratio,
        'func_score': func_score,
        'punct_density': punct_density,
        'punct_score': punct_score,
        'literary_penalty': literary_penalty,
        'literary_score': literary_score,
        'romantic_penalty': romantic_penalty,
        'romantic_score': romantic_score,
        'question_ratio': question_ratio,
        'humanization_score': humanization_score,
        'struct_score': struct_score,
        'total_score': ai_score,
        'sentence_count': len(sentences),
        'word_count': len(words),
    }
    
    return ai_prob, details


# ==================== æ¸¬è©¦ç”¨ä¾‹ ====================

# ç”¨æˆ¶æä¾›çš„è‹±æ–‡æ„›æƒ…å°èªª
ai_romance = """The city lights were blurred watercolors against the glass, but inside the small room, only the amber glow of the fireplace held the darkness at bay.

She traced the sharp line of his jaw with her thumb, a gesture so familiar it felt like breathing. "You look worried," she murmured, her voice barely a whisper against the crackle of the wood.

He turned his face into her hand, kissing her palm softly. "I'm only worried that a moment this perfect can't last," he admitted, his eyes holding hersâ€”a deep, troubled blue. "It feels like borrowed time."

"It's not borrowed," she countered, shifting closer so their foreheads touched. The scent of rain and old books clung to his sweater, a scent she had come to associate with home. "It's ours. We built this stillness, didn't we? Out of all the chaos and the years apart."

A slow, undeniable smile broke through his pensive expression. He wrapped his arms around her, pulling her completely onto his lap.

"Then let's make a promise," he whispered, his lips close to her ear. "No matter what storms are waiting, no matter how loud the world gets, we find this room again. Every time. We find the fire, and we find this silence."

"I promise," she replied, tightening her embrace, the warmth of the fire now the least of the heat between them. She knew then that love wasn't about fireworks; it was about the solidity of a quiet vow made under the gentle surveillance of the moon."""

print("="*70)
print("ğŸ§ª è‹±æ–‡æ„›æƒ…å°èªªæª¢æ¸¬æ¸¬è©¦ - ç‰ˆæœ¬ 8 (å¹³è¡¡ç‰ˆ)")
print("="*70)
print(f"\næ–‡æœ¬é•·åº¦: {len(ai_romance)} å­—ç¬¦\n")

# ç‰ˆæœ¬ 8 - å¹³è¡¡ç‰ˆ
ai_prob_v8, details_v8 = analyze_text_v8_balanced(ai_romance)

print("ç‰ˆæœ¬ 8 (è‹±æ–‡æ„›æƒ…å°èªªå¹³è¡¡ç‰ˆ):")
print("-" * 70)
print(f"ğŸ“Š æœ€çµ‚ AI è©•åˆ†: {ai_prob_v8*100:.2f}%")
print(f"ğŸ’­ åˆ¤å®š: {'ğŸ¤– AI ç”Ÿæˆ' if ai_prob_v8 > 0.50 else 'ğŸ‘¤ äººé¡æ’°å¯«'}")
print()
print("è©³ç´°åˆ†è§£:")
print(f"  1ï¸âƒ£  è©å½™å¤šæ¨£æ€§ (TTR): {details_v8['vocab_ratio']:.3f} â†’ {details_v8['ttr_score']:.4f} (23%)")
print(f"  2ï¸âƒ£  å¥å­ä¸€è‡´æ€§ (CV):  {details_v8['cv']:.3f} â†’ {details_v8['consistency_score']:.4f} (23%)")
print(f"  3ï¸âƒ£  è‹±æ–‡åŠŸèƒ½è©:      {details_v8['func_ratio']:.1%} â†’ {details_v8['func_score']:.4f} (8%)")
print(f"  4ï¸âƒ£  æ¨™é»ç¬¦è™Ÿ:        {details_v8['punct_density']:.1%} â†’ {details_v8['punct_score']:.4f} (6%)")
print(f"  5ï¸âƒ£  æ–‡å­¸æ¨™è¨˜:        æ‡²ç½° {details_v8['literary_penalty']:.3f} â†’ {details_v8['literary_score']:.4f} (18%)")
print(f"  6ï¸âƒ£  æµªæ¼«æ¨™è¨˜:        æ‡²ç½° {details_v8['romantic_penalty']:.3f} â†’ {details_v8['romantic_score']:.4f} (15%)")
print(f"  7ï¸âƒ£  äººæ€§åŒ–ç‰¹å¾µ:      æ‡²ç½° {-details_v8['humanization_score']:.3f} â†’ {details_v8['humanization_score']:.4f} (8%)")
print(f"  8ï¸âƒ£  çµæ§‹è¦å¾‹:        {details_v8['struct_score']:.4f} (6%)")
print()
print(f"ç¸½è¨ˆ: {details_v8['total_score']:.4f} = {ai_prob_v8*100:.2f}%")
print()

# åˆ†æå•é¡Œ
print("="*70)
print("ğŸ” çµæœåˆ†æ")
print("="*70)
print(f"ç”¨æˆ¶å ±å‘Š:   56% ç‚ºäººé¡ (æ‡‰è©²æ˜¯ > 50% AI)")
print(f"ç‰ˆæœ¬ 8 çµæœ: {ai_prob_v8*100:.2f}%")
if ai_prob_v8 > 0.50:
    print(f"ç‹€æ…‹: âœ… æ­£ç¢º! å·²è­˜åˆ¥ç‚º AI ç”Ÿæˆæ–‡æœ¬")
elif ai_prob_v8 > 0.45:
    print(f"ç‹€æ…‹: ğŸŸ¡ æ¥è¿‘é‚Šç•Œ (44-50% ç¯„åœ)")
else:
    print(f"ç‹€æ…‹: âœ— ä»éœ€æ”¹é€² (ç›®å‰ {ai_prob_v8*100:.2f}%, éœ€è¦ > 50%)")

print()
print("æ¬Šé‡çµæ§‹å°æ¯”:")
print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print("â”‚  ç¶­åº¦              v7        v8      èªªæ˜        â”‚")
print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
print("â”‚  è©å½™å¤šæ¨£æ€§       23%       23%              â”‚")
print("â”‚  å¥å­ä¸€è‡´æ€§       23%       23%              â”‚")
print("â”‚  åŠŸèƒ½è©           8%        8%              â”‚")
print("â”‚  æ¨™é»ç¬¦è™Ÿ         6%        6%              â”‚")
print("â”‚  æ–‡å­¸æ¨™è¨˜        27%       18%    â†“ é™ä½9%  â”‚")
print("â”‚  æµªæ¼«æ¨™è¨˜        (ç„¡)      15%    â†‘ æ–°å¢    â”‚")
print("â”‚  äººæ€§åŒ–ç‰¹å¾µ       8%        8%              â”‚")
print("â”‚  çµæ§‹è¦å¾‹         6%        6%              â”‚")
print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
print()
