#!/usr/bin/env python3
"""
æ”¹é€²çš„è©•åˆ†æ¸¬è©¦ - åŒ…å«æ–‡å­¸é¢¨æ ¼æª¢æ¸¬
æ”¯æ´ç¶“å…¸æ–‡å­¸ä½œå“è­˜åˆ¥
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np

# æ¸¬è©¦æ–‡æœ¬

# AI ç”Ÿæˆçš„æ–‡æœ¬
ai_text = """
The development of artificial intelligence has become increasingly significant in recent years. 
The technological advancements have led to numerous applications across various industries. 
The implementation of machine learning algorithms has resulted in improved efficiency and accuracy. 
The analysis of data has become more sophisticated and comprehensive. 
The future of technology appears to be closely linked with artificial intelligence.
"""

# äººé¡å¯«çš„æ–‡æœ¬
human_text = """
ä½ çŸ¥é“å—ï¼Œæˆ‘æœ€è¿‘åœ¨æƒ³ä¸€å€‹å•é¡Œã€‚ç‚ºä»€éº¼æœ‰äº›äººå°±æ˜¯ç‰¹åˆ¥æ“…é•·å¯«æ±è¥¿ï¼Ÿ
å¯èƒ½æ˜¯å› ç‚ºä»–å€‘è®€å¾—å¤šï¼Œæˆ–è€…å°±æ˜¯å¤©ç”Ÿçš„æ‰è¯å§ã€‚ä¸éè©±èªªå›ä¾†ï¼Œ
å¯«å¥½æ±è¥¿çœŸçš„ä¸å®¹æ˜“ã€‚è¦æŠŠæƒ³æ³•æ¸…æ¥šåœ°è¡¨é”å‡ºä¾†ï¼Œé‚„è¦è®“äººæ„Ÿèˆˆè¶£ï¼Œ
é€™éœ€è¦æ™‚é–“å’Œç·´ç¿’ã€‚æˆ‘è¦ºå¾—æœ€é‡è¦çš„æ˜¯è¦æœ‰çœŸå¯¦çš„æƒ³æ³•ï¼Œ
è€Œä¸æ˜¯æ©Ÿæ¢°åœ°æ‹¼æ¹Šè©å½™ã€‚ä½ åŒæ„å—ï¼Ÿ
"""

# é­¯è¿…ã€Šç‹‚äººæ—¥è¨˜ã€‹ç‰‡æ®µ
luxun_text = """
æˆ‘ç¿»é–‹æ­·å²ä¸€æŸ¥ï¼Œé€™æ­·å²æ²’æœ‰å¹´ä»£ï¼Œæ­ªæ­ªæ–œæ–œçš„æ¯è‘‰ä¸Šéƒ½å¯«è‘—"ä»ç¾©é“å¾·"å¹¾å€‹å­—ã€‚
æˆ‘æ©«è±ç¡ä¸è‘—ï¼Œä»”ç´°çœ‹äº†åŠå¤œï¼Œæ‰å¾å­—ç¸«è£¡çœ‹å‡ºå­—ä¾†ï¼Œæ»¿æœ¬éƒ½å¯«è‘—å…©å€‹å­—æ˜¯"åƒäºº"ï¼
æˆ‘é€™å›å¯æ˜¯çœŸçš„è¢«åš‡å£äº†ï¼›è¶•ç·Šåˆä¸Šæ­·å²ï¼›å¿ƒè£¡å»çªç„¶ä¸€é™£å¾ˆå†·çš„æˆ°æ…„ã€‚
"""

def score_text(text):
    """ä½¿ç”¨æ”¹é€²çš„è©•åˆ†é‚è¼¯ - åŒ…å«æ–‡å­¸é¢¨æ ¼æª¢æ¸¬"""
    ai_score = 0
    score_factors = {}
    
    # æ¸…ç†æ–‡æœ¬
    words = text.split()
    words_lower = [w.lower() for w in words]
    
    # 1. è©å½™å¤šæ¨£æ€§ (35%)
    if len(words_lower) > 0:
        unique_words = len(set(words_lower))
        vocab_ratio = unique_words / len(words_lower)
        vocab_score = max(0, min((vocab_ratio - 0.5) / 0.3, 1)) * 0.35
        ai_score += vocab_score
        score_factors['vocabulary_diversity'] = vocab_score
    
    # 2. å¥å­ä¸€è‡´æ€§ (35%)
    sentences = [s.strip() for s in 
               text.replace('ã€‚', '.|').replace('ï¼', '!|').replace('ï¼Ÿ', '?|')
                   .replace('.', '.|').replace('!', '!|').replace('?', '?|')
                   .split('|') if s.strip()]
    if len(sentences) > 1:
        sent_lengths = [len(s.split()) for s in sentences]
        mean_len = np.mean(sent_lengths)
        std_len = np.std(sent_lengths)
        cv = std_len / (mean_len + 1e-6) if mean_len > 0 else 0
        consistency_score = max(0, 1 - min(cv, 1.2) / 1.2) * 0.35
        ai_score += consistency_score
        score_factors['sentence_consistency'] = consistency_score
    
    # 3. åŠŸèƒ½è© (10%)
    if any(ord(c) < 128 for c in text):
        function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'of',
                         'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had'}
        if len(words_lower) > 0:
            func_word_count = sum(1 for w in words_lower if w in function_words)
            func_ratio = func_word_count / len(words_lower)
            func_score = min(func_ratio / 0.30, 1) * 0.10
            ai_score += func_score
            score_factors['function_words'] = func_score
    
    # 4. æ¨™é» (10%)
    punct_chars = '.,!?;:\'"â€”-ã€‚ï¼ï¼Ÿï¼›ï¼š''""'
    total_punct = sum(1 for c in text if c in punct_chars)
    punct_density = total_punct / max(len(text), 1)
    
    if punct_density < 0.015:
        punct_score = 0.0
    elif punct_density < 0.03:
        punct_score = 0.05
    else:
        punct_score = 0.10
    
    ai_score += punct_score
    score_factors['punctuation_pattern'] = punct_score
    
    # 5. æ–‡å­¸é¢¨æ ¼æª¢æ¸¬ (15%) - æ–°å¢
    # AI é€šå¸¸é¿å…ä½¿ç”¨æ–‡å­¸æ€§æªè¾­ã€æ¯”å–»ã€æ“¬äººæ³•ç­‰
    literary_markers = {
        # ä¸­æ–‡æ–‡å­¸è©å½™
        'ç„¶è€Œ', 'æ—¢ç„¶', 'è«è‹¥', 'å…¶å¯¦', 'æ³ä¸”', 'è€Œæ³', 'ä¸æ–™', 'è±ˆæ–™',
        'æƒ³ä¸åˆ°', 'æ€æ–™', 'èª°çŸ¥', 'å»', 'ç«Ÿ', 'ç«Ÿç„¶', 'åå', 'æ°å¥½',
        'æ°æ°', 'æ­£å¥½', 'æ¹Šå·§', 'æ€ªä¸å¾—', 'ä¹Ÿé›£æ€ª', 'ä¹Ÿæ€ªå¾—',
        'æ‚²å“€', 'æ·’æ¶¼', 'è’¼æ¶¼', 'è’æ¶¼', 'å¯‚å¯¥', 'å­¤å¯‚', 'é ¹å»¢',
        'è¿·èŒ«', 'å›°é “', 'æŠ‘é¬±', 'æ²‰æ‚¶', 'å£“æŠ‘', 'çª’æ¯',
        'å‘¢å–ƒ', 'ä½è²', 'è¼•è²', 'ç´°èª', 'å–ƒå–ƒ', 'å›ˆèª',
        'æ±¨æ±¨', 'æ½ºæ½º', 'æ·™æ·™', 'æ‚ æ‚ ', 'æ‚„æ‚„', 'é»˜é»˜',
        'é£„é£„ç„¶', 'æ¸ºæ¸ºç„¶', 'èŒ«èŒ«ç„¶', 'æ†‘æ·»', 'æ·»å¢', 'è¡ç”Ÿ',
        'æ©«è±', 'ä»”ç´°', 'æˆ°æ…„', 'æ­ªæ­ªæ–œæ–œ', 'å­—ç¸«', 'åƒäºº',
        # è‹±æ–‡æ–‡å­¸è©å½™
        'alas', 'behold', 'hark', 'lo', 'methinks', 'perchance',
        'forsooth', 'thus', 'verily', 'hence', 'whence', 'thence',
        'woe', 'sorrow', 'anguish', 'melancholy', 'languish',
    }
    
    literary_count = 0
    for marker in literary_markers:
        if marker in text.lower():
            literary_count += 1
    
    # æ–‡å­¸è©å½™è¶Šå¤šï¼Œè¶Šä¸åƒ AI (æ‰€ä»¥æ¸›ä½åˆ†æ•¸)
    literary_density = literary_count / max(len(words_lower) / 100, 1)
    # é«˜æ–‡å­¸å¯†åº¦ -> ä½ AI åˆ†æ•¸
    literary_score = max(0, 1 - min(literary_density * 0.5, 1)) * 0.15
    ai_score -= literary_score * 0.5  # æ¸›ä½ AI åˆ†æ•¸
    score_factors['literary_style'] = -literary_score * 0.5
    
    # 6. çµæ§‹ (5%)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    if len(paragraphs) <= 1:
        ai_score += 0.02
        score_factors['structure'] = 0.02
    else:
        para_lengths = [len(p.split()) for p in paragraphs]
        para_std = np.std(para_lengths)
        para_mean = np.mean(para_lengths)
        para_cv = para_std / (para_mean + 1e-6) if para_mean > 0 else 0
        struct_score = max(0, 1 - min(para_cv, 1)) * 0.05
        ai_score += struct_score
        score_factors['structure'] = struct_score
    
    ai_prob = max(0, min(ai_score, 1.0))
    return ai_prob, score_factors


# é¡è‰²å®šç¾©
GREEN = '\033[92m'
RED = '\033[91m'
BLUE = '\033[94m'
YELLOW = '\033[93m'
END = '\033[0m'

print(f"\n{BLUE}{'='*70}")
print("  AI åµæ¸¬ç³»çµ± - æ”¹é€²ç‰ˆè©•åˆ†æ¸¬è©¦ (å«æ–‡å­¸é¢¨æ ¼æª¢æ¸¬)")
print(f"{'='*70}{END}\n")

test_cases = [
    ("ğŸ¤– AI ç”Ÿæˆæ–‡æœ¬", ai_text, True),
    ("ğŸ‘¤ äººé¡æ–‡æœ¬", human_text, False),
    ("ğŸ“š é­¯è¿…ã€Šç‹‚äººæ—¥è¨˜ã€‹", luxun_text, False),
]

print("æ¸¬è©¦çµæœ:\n")

results = []
for name, text, is_ai in test_cases:
    ai_prob, factors = score_text(text)
    results.append((name, ai_prob, is_ai))
    
    # åˆ¤å®š
    if is_ai and ai_prob > 0.5:
        status = f"{GREEN}âœ… æ­£ç¢º{END}"
    elif not is_ai and ai_prob < 0.5:
        status = f"{GREEN}âœ… æ­£ç¢º{END}"
    else:
        status = f"{YELLOW}âš ï¸  èª¤åˆ¤{END}"
    
    print(f"{name:20} â†’ {ai_prob:6.2%} AI {status}")
    print(f"   åˆ†æ•¸å› å­:")
    for k, v in factors.items():
        if v != 0:
            print(f"     - {k:25} = {v:+.3f}")
    print()

# é©—è­‰çµæœ
print(f"{BLUE}{'='*70}{END}")
print("é©—è­‰çµæœ:\n")

success = 0
total = len(test_cases)

for name, ai_prob, is_ai in results:
    if is_ai and ai_prob > 0.5:
        print(f"{GREEN}âœ…{END} {name:20} â†’ {ai_prob:.2%} AI (æ­£ç¢º)")
        success += 1
    elif not is_ai and ai_prob < 0.5:
        print(f"{GREEN}âœ…{END} {name:20} â†’ {ai_prob:.2%} AI (æ­£ç¢º)")
        success += 1
    else:
        print(f"{RED}âŒ{END} {name:20} â†’ {ai_prob:.2%} AI (èª¤åˆ¤)")

print(f"\næ¸¬è©¦é€šéç‡: {success}/{total} ({100*success//total}%)")

if success == total:
    print(f"\n{GREEN}âœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼æ–‡å­¸é¢¨æ ¼æª¢æ¸¬å·¥ä½œæ­£å¸¸ï¼{END}\n")
else:
    print(f"\n{YELLOW}âš ï¸  éƒ¨åˆ†æ¸¬è©¦æœªé€šéï¼Œè«‹æª¢æŸ¥è©•åˆ†é‚è¼¯{END}\n")
