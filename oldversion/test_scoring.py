#!/usr/bin/env python3
"""
æ¸¬è©¦æ”¹é€²çš„è©•åˆ†é‚è¼¯
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np

# AI ç”Ÿæˆçš„æ–‡æœ¬
ai_text = """
The development of artificial intelligence has become increasingly significant in recent years. 
The technological advancements have led to numerous applications across various industries. 
The implementation of machine learning algorithms has resulted in improved efficiency and accuracy. 
The analysis of data has become more sophisticated and comprehensive. 
The future of technology appears to be closely linked with artificial intelligence.
"""

# äººé¡å¯«çš„æ–‡æœ¬
human_text = """
ä½ çŸ¥é“å—ï¼Œæˆ‘æœ€è¿‘åœ¨æƒ³ä¸€å€‹å•é¡Œã€‚ç‚ºä»€éº¼æœ‰äº›äººå°±æ˜¯ç‰¹åˆ¥æ“…é•·å¯«æ±è¥¿ï¼Ÿ
å¯èƒ½æ˜¯å› ç‚ºä»–å€‘è®€å¾—å¤šï¼Œæˆ–è€…å°±æ˜¯å¤©ç”Ÿçš„æ‰è¯å§ã€‚ä¸éè©±èªªå›ä¾†ï¼Œ
å¯«å¥½æ±è¥¿çœŸçš„ä¸å®¹æ˜“ã€‚è¦æŠŠæƒ³æ³•æ¸…æ¥šåœ°è¡¨é”å‡ºä¾†ï¼Œé‚„è¦è®“äººæ„Ÿèˆˆè¶£ï¼Œ
é€™éœ€è¦æ™‚é–“å’Œç·´ç¿’ã€‚æˆ‘è¦ºå¾—æœ€é‡è¦çš„æ˜¯è¦æœ‰çœŸå¯¦çš„æƒ³æ³•ï¼Œ
è€Œä¸æ˜¯æ©Ÿæ¢°åœ°æ‹¼æ¹Šè©å½™ã€‚ä½ åŒæ„å—ï¼Ÿ
"""

def score_text(text):
    """ä½¿ç”¨å¼·åŒ–çš„è©•åˆ†é‚è¼¯ - æœ€å„ªåŒ–ç‰ˆæœ¬"""
    ai_score = 0
    score_factors = {}
    
    # æ¸…ç†å’Œæº–å‚™æ–‡æœ¬
    words = text.split()
    words_lower = [w.lower() for w in words]
    text_lower = text.lower()
    
    # 1. è©é‡è¤‡åº¦ (æœ€é‡è¦çš„æŒ‡æ¨™) - æ¬Šé‡æå‡åˆ° 40%
    if len(words_lower) > 0:
        unique_words = len(set(words_lower))
        vocab_ratio = unique_words / len(words_lower)  # TTR: Type-Token Ratio
        # AI: TTR é«˜ (0.6-0.8) -> é«˜åˆ†
        # äººé¡: TTR ä½ (0.4-0.6) -> ä½åˆ†
        # å°‡ TTR 0.4-0.8 æ˜ å°„åˆ° 0-1
        vocab_score = max(0, min((vocab_ratio - 0.4) / 0.4, 1)) * 0.40
        ai_score += vocab_score
        score_factors['vocabulary_diversity'] = vocab_score
    
    # 2. æ–‡æœ¬æµæš¢æ€§ - å¥å­é•·åº¦è®Šç•°ä¿‚æ•¸ (30%)
    sentences = [s.strip() for s in 
               text.replace('ã€‚', '.|').replace('ï¼', '!|').replace('ï¼Ÿ', '?|')
                   .replace('.', '.|').replace('!', '!|').replace('?', '?|')
                   .split('|') if s.strip()]
    if len(sentences) > 1:
        sent_lengths = [len(s.split()) for s in sentences]
        mean_len = np.mean(sent_lengths)
        std_len = np.std(sent_lengths)
        cv = std_len / (mean_len + 1e-6) if mean_len > 0 else 0
        # CV < 0.3 = äººé¡è‡ªç„¶, CV > 0.8 = AI æ©Ÿæ¢°
        # åè½‰é‚è¼¯ï¼šé«˜ CV = äººé¡, ä½ CV = AI
        consistency_score = max(0, 1 - min(cv, 1.2) / 1.2) * 0.30
        ai_score += consistency_score
        score_factors['sentence_consistency'] = consistency_score
    
    # 3. è©é¡åˆ†å¸ƒ (ä½æ¬Šé‡ï¼Œå› ç‚ºèªè¨€å·®ç•°) - 15%
    # åªåœ¨è‹±æ–‡æ™‚æœ‰æ•ˆ
    if any(ord(c) < 128 for c in text):  # æª¢æ¸¬æ˜¯å¦æœ‰è‹±æ–‡å­—ç¬¦
        function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'of',
                         'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had'}
        if len(words_lower) > 0:
            func_word_count = sum(1 for w in words_lower if w in function_words)
            func_ratio = func_word_count / len(words_lower)
            func_score = min(func_ratio / 0.30, 1) * 0.15
            ai_score += func_score
            score_factors['function_words'] = func_score
    
    # 4. æ¨™é»ç¬¦è™Ÿæ¨¡å¼ (10%)
    punct_chars = '.,!?;:\'"â€”-ã€‚ï¼ï¼Ÿï¼›ï¼š''""'
    total_punct = sum(1 for c in text if c in punct_chars)
    punct_density = total_punct / max(len(text), 1)
    
    # AI æ–‡æœ¬æ¨™é»å¯†åº¦é€šå¸¸ 0.02-0.04ï¼Œäººé¡ 0.01-0.03
    if punct_density < 0.015:
        punct_score = 0.0  # äººé¡å‚¾å‘
    elif punct_density < 0.03:
        punct_score = 0.05
    else:
        punct_score = 0.10  # AI å‚¾å‘
    
    ai_score += punct_score
    score_factors['punctuation_pattern'] = punct_score
    
    # 5. æ²’æœ‰æ®µè½çµæ§‹ (5%)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    if len(paragraphs) <= 1:
        # å–®æ®µè½ = å¯èƒ½ AI
        ai_score += 0.05
        score_factors['structure'] = 0.05
    else:
        # æœ‰å¤šæ®µè½ = æ›´åƒäººé¡
        para_lengths = [len(p.split()) for p in paragraphs]
        para_std = np.std(para_lengths)
        para_mean = np.mean(para_lengths)
        para_cv = para_std / (para_mean + 1e-6) if para_mean > 0 else 0
        # ä½è®Šç•° = AI
        struct_score = max(0, 1 - min(para_cv, 1)) * 0.05
        ai_score += struct_score
        score_factors['structure'] = struct_score
    
    ai_prob = max(0, min(ai_score, 1.0))
    return ai_prob, score_factors

print("=" * 60)
print("AI åµæ¸¬ç³»çµ± - è©•åˆ†é‚è¼¯æ¸¬è©¦")
print("=" * 60)

print("\nğŸ“Š AI ç”Ÿæˆçš„æ–‡æœ¬è©•åˆ†ï¼š")
ai_prob_ai, factors_ai = score_text(ai_text)
print(f"AI æ¦‚ç‡: {ai_prob_ai:.2%}")
print(f"åˆ†æ•¸å› å­ï¼š{factors_ai}")
print(f"åˆ¤å®šçµæœ: {'ğŸ¤– AI ç”Ÿæˆ' if ai_prob_ai >= 0.5 else 'ğŸ‘¤ äººé¡æ’°å¯«'}")

print("\n" + "=" * 60)
print("\nğŸ“Š äººé¡å¯«çš„æ–‡æœ¬è©•åˆ†ï¼š")
ai_prob_human, factors_human = score_text(human_text)
print(f"AI æ¦‚ç‡: {ai_prob_human:.2%}")
print(f"åˆ†æ•¸å› å­ï¼š{factors_human}")
print(f"åˆ¤å®šçµæœ: {'ğŸ¤– AI ç”Ÿæˆ' if ai_prob_human >= 0.5 else 'ğŸ‘¤ äººé¡æ’°å¯«'}")

print("\n" + "=" * 60)
print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼")
print(f"é æœŸï¼šAI æ–‡æœ¬ > 50%, äººé¡æ–‡æœ¬ < 50%")
print(f"å¯¦éš›ï¼šAI {ai_prob_ai:.2%}, äººé¡ {ai_prob_human:.2%}")

if ai_prob_ai > ai_prob_human:
    print("âœ… è©•åˆ†é‚è¼¯æ­£å¸¸å·¥ä½œï¼")
else:
    print("âš ï¸ è©•åˆ†é‚è¼¯éœ€è¦èª¿æ•´")
