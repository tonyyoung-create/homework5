"""
é©—è­‰ app.py ä¿®å¾©å¾Œçš„è©•åˆ†
è¤‡è£½ app.py çš„è©•åˆ†é‚è¼¯é€²è¡Œæ¸¬è©¦
"""

import re
import numpy as np

def analyze_text_app_fixed(input_text):
    """å¾ app.py è¤‡è£½ä¸¦ç¨ä½œèª¿æ•´çš„è©•åˆ†å‡½æ•¸"""
    
    ai_score = 0
    score_factors = {}
    
    # 1. è©å½™å¤šæ¨£æ€§ (23%)
    words = input_text.split()
    words_lower = [w.lower() for w in words]
    
    if len(words_lower) > 0:
        unique_words = len(set(words_lower))
        vocab_ratio = unique_words / len(words_lower)
        vocab_score = max(0, min((vocab_ratio - 0.54) / 0.26, 1)) * 0.23
        ai_score += vocab_score
        score_factors['vocabulary_diversity'] = vocab_score
    
    # 2. å¥å­ä¸€è‡´æ€§ (23%)
    sentences = [s.strip() for s in 
               input_text.replace('ã€‚', '.|').replace('ï¼', '!|').replace('ï¼Ÿ', '?|')
                       .replace('.', '.|').replace('!', '!|').replace('?', '?|')
                       .split('|') if s.strip()]
    if len(sentences) > 1:
        sent_lengths = [len(s.split()) for s in sentences]
        mean_len = np.mean(sent_lengths)
        std_len = np.std(sent_lengths)
        cv = std_len / (mean_len + 1e-6) if mean_len > 0 else 0
        consistency_score = max(0, 1 - min(cv, 1.3) / 1.3) * 0.23
        ai_score += consistency_score
        score_factors['sentence_consistency'] = consistency_score
    
    # 3. è‹±æ–‡åŠŸèƒ½è© (8%)
    if any(ord(c) < 128 for c in input_text):
        function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'of',
                        'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had'}
        if len(words_lower) > 0:
            func_word_count = sum(1 for w in words_lower if w in function_words)
            func_ratio = func_word_count / len(words_lower)
            func_score = min(func_ratio / 0.30, 1) * 0.08
            ai_score += func_score
            score_factors['function_words'] = func_score
    
    # 4. æ¨™é»ç¬¦è™Ÿå¯†åº¦ (6%)
    punct_chars = '.,!?;:\'"â€”-ã€‚ï¼ï¼Ÿï¼›ï¼š''""'
    total_punct = sum(1 for c in input_text if c in punct_chars)
    punct_density = total_punct / max(len(input_text), 1)
    
    if punct_density < 0.015:
        punct_score = 0.0
    elif punct_density < 0.03:
        punct_score = 0.03
    else:
        punct_score = 0.06
    
    ai_score += punct_score
    score_factors['punctuation_pattern'] = punct_score
    
    # 5. å¤æ–‡/ç¶“å…¸æ–‡å­¸æª¢æ¸¬ (20%) - ä¿®å¾©ç‰ˆ
    classical_literary_markers = {
        # ä¸­æ–‡å¤æ–‡è©å½™
        'ç„¶è€Œ', 'æ—¢ç„¶', 'è«è‹¥', 'å…¶å¯¦', 'æ³ä¸”', 'è€Œæ³', 'ä¸æ–™', 'è±ˆæ–™',
        'æƒ³ä¸åˆ°', 'æ€æ–™', 'èª°çŸ¥', 'å»', 'ç«Ÿ', 'ç«Ÿç„¶', 'åå', 'æ°å¥½',
        'æ°æ°', 'æ­£å¥½', 'æ¹Šå·§', 'æ€ªä¸å¾—', 'ä¹Ÿé›£æ€ª', 'ä¹Ÿæ€ªå¾—',
        'æ©«è±', 'ä»”ç´°', 'æˆ°æ…„', 'æ­ªæ­ªæ–œæ–œ', 'åƒäºº', 'å­—ç¸«',
        'ç¿»é–‹', 'æ­·å²', 'ä»ç¾©', 'é“å¾·', 'æ»¿æœ¬',
        # è‹±æ–‡å¤å…¸è©å½™
        'alas', 'behold', 'hark', 'lo', 'methinks', 'perchance',
        'forsooth', 'thus', 'verily', 'hence', 'whence', 'thence',
        'thee', 'thou', 'thy', 'hath', 'doth', 'wherefore',
    }
    
    classical_count = 0
    text_lower = input_text.lower()
    
    for marker in classical_literary_markers:
        if '\u4e00' <= marker[0] <= '\u9fff':
            if marker in text_lower:
                classical_count += 1
        elif ord(marker[0]) < 128:
            if re.search(r'\b' + marker + r'\b', text_lower):
                classical_count += 1
    
    if classical_count > 0:
        classical_penalty = min(classical_count * 0.25, 0.50)
        ai_score -= classical_penalty
        score_factors['literary_style'] = -classical_penalty
    else:
        score_factors['literary_style'] = 0.0
    
    # 5.5 æµªæ¼«å…§å®¹æª¢æ¸¬ (ä¸æ‡²ç½°)
    romantic_emotional_words = {
        'love', 'heart', 'smile', 'warmth', 'embrace', 'promise',
        'fire', 'silence', 'perfect', 'familiar', 'softly', 'closer',
        'traced', 'whisper', 'blurred', 'watercolors', 'amber', 'glow',
        'breathing', 'murmured', 'kissing', 'admitted', 'troubled',
        'borrowed', 'countered', 'foreheads', 'scent', 'clung',
        'sweater', 'stillness', 'chaos', 'undeniable', 'pensive',
        'wrapped', 'completely', 'whispered', 'storms', 'waiting',
        'tightening', 'moon', 'vow', 'fireworks', 'solidity',
        'surveillance', 'tender', 'gentle', 'passionate', 'desire',
        'longing', 'yearning', 'adore', 'cherish', 'beloved',
    }
    
    romantic_count = 0
    words_list = input_text.lower().split()
    for word in words_list:
        clean_word = word.strip('.,!?;:\'"')
        if clean_word in romantic_emotional_words:
            romantic_count += 1
    
    score_factors['romantic_content'] = romantic_count
    
    # 6. äººæ€§åŒ–æ¨™è¨˜ (7%)
    humanization_score = 0
    
    question_count = input_text.count('?') + input_text.count('ï¼Ÿ')
    question_ratio = question_count / max(len(sentences), 1)
    if question_ratio > 0.15:
        humanization_score += 0.035
    
    ellipsis_count = input_text.count('...') + input_text.count('ã€‚ã€‚ã€‚')
    if ellipsis_count > 0:
        humanization_score += 0.02
    
    personal_words = {'æˆ‘', 'æˆ‘è¦ºå¾—', 'æˆ‘èªç‚º', 'æˆ‘æƒ³', 'æˆ‘ç™¼ç¾', 'æˆ‘çœ‹', 
                    'i think', 'i feel', 'i believe', 'in my opinion'}
    personal_count = 0
    for word in personal_words:
        if '\u4e00' <= word[0] <= '\u9fff':
            if word in text_lower:
                personal_count += 1
        else:
            if re.search(r'\b' + word + r'\b', text_lower):
                personal_count += 1
    
    if personal_count > 1:
        humanization_score += min(personal_count * 0.015, 0.015)
    
    ai_score -= min(humanization_score, 0.07)
    score_factors['humanization'] = -min(humanization_score, 0.07)
    
    # 7. çµæ§‹è¦å¾‹æ€§ (8%)
    paragraphs = [p.strip() for p in input_text.split('\n\n') if p.strip()]
    if len(paragraphs) <= 1:
        ai_score += 0.04
        score_factors['structure'] = 0.04
    else:
        para_lengths = [len(p.split()) for p in paragraphs]
        para_std = np.std(para_lengths)
        para_mean = np.mean(para_lengths)
        para_cv = para_std / (para_mean + 1e-6) if para_mean > 0 else 0
        struct_score = max(0, 1 - min(para_cv, 1)) * 0.08
        ai_score += struct_score
        score_factors['structure'] = struct_score
    
    # ç¢ºä¿åˆ†æ•¸åœ¨ [0, 1] ç¯„åœå…§
    ai_prob = max(0, min(ai_score, 1.0))
    
    return ai_prob, score_factors


# ==================== æ¸¬è©¦ç”¨ä¾‹ ====================

ai_romance = """The city lights were blurred watercolors against the glass, but inside the small room, only the amber glow of the fireplace held the darkness at bay.

She traced the sharp line of his jaw with her thumb, a gesture so familiar it felt like breathing. "You look worried," she murmured, her voice barely a whisper against the crackle of the wood.

He turned his face into her hand, kissing her palm softly. "I'm only worried that a moment this perfect can't last," he admitted, his eyes holding hersâ€”a deep, troubled blue. "It feels like borrowed time."

"It's not borrowed," she countered, shifting closer so their foreheads touched. The scent of rain and old books clung to his sweater, a scent she had come to associate with home. "It's ours. We built this stillness, didn't we? Out of all the chaos and the years apart."

A slow, undeniable smile broke through his pensive expression. He wrapped his arms around her, pulling her completely onto his lap.

"Then let's make a promise," he whispered, his lips close to her ear. "No matter what storms are waiting, no matter how loud the world gets, we find this room again. Every time. We find the fire, and we find this silence."

"I promise," she replied, tightening her embrace, the warmth of the fire now the least of the heat between them. She knew then that love wasn't about fireworks; it was about the solidity of a quiet vow made under the gentle surveillance of the moon."""

print("="*70)
print("ğŸ§ª ä¿®å¾©ç‰ˆapp.pyæ¸¬è©¦ - è‹±æ–‡æ„›æƒ…å°èªª")
print("="*70)

ai_prob, factors = analyze_text_app_fixed(ai_romance)

print(f"\nğŸ“Š æœ€çµ‚ AI è©•åˆ†: {ai_prob*100:.2f}%")
print(f"ğŸ’­ åˆ¤å®š: {'ğŸ¤– AI ç”Ÿæˆ' if ai_prob > 0.50 else 'ğŸ‘¤ äººé¡æ’°å¯«'}")
print()
print("è©³ç´°åˆ†è§£:")
print(f"  1ï¸âƒ£  è©å½™å¤šæ¨£æ€§:           {factors.get('vocabulary_diversity', 0):.4f} (23%)")
print(f"  2ï¸âƒ£  å¥å­ä¸€è‡´æ€§:           {factors.get('sentence_consistency', 0):.4f} (23%)")
print(f"  3ï¸âƒ£  è‹±æ–‡åŠŸèƒ½è©:           {factors.get('function_words', 0):.4f} (8%)")
print(f"  4ï¸âƒ£  æ¨™é»ç¬¦è™Ÿ:             {factors.get('punctuation_pattern', 0):.4f} (6%)")
print(f"  5ï¸âƒ£  å¤æ–‡/ç¶“å…¸æ–‡å­¸:        {factors.get('literary_style', 0):.4f} (20%)")
print(f"  6ï¸âƒ£  äººæ€§åŒ–æ¨™è¨˜:           {factors.get('humanization', 0):.4f} (7%)")
print(f"  7ï¸âƒ£  çµæ§‹è¦å¾‹:             {factors.get('structure', 0):.4f} (8%)")
print(f"  ğŸ­ æµªæ¼«å…§å®¹è©èªæ•¸:        {factors.get('romantic_content', 0)} (æœªæ‡²ç½°)")
print()
print(f"  é æœŸ: > 50% (AI ç”Ÿæˆ)")
print(f"  çµæœ: {ai_prob*100:.2f}% {'âœ…' if ai_prob > 0.50 else 'âŒ'}")
print()

# æ¸¬è©¦é­¯è¿…æ–‡æœ¬ä»¥ç¢ºä¿æœªè¿´æ­¸
luxun_text = """æˆ‘ç¿»é–‹æ­·å²ä¸€æŸ¥ï¼Œé€™æ­·å²æ²’æœ‰å¹´ä»£ï¼Œæ­ªæ­ªæ–œæ–œçš„æ¯è‘‰ä¸Šéƒ½å¯«è‘—"ä»ç¾©é“å¾·"å¹¾å€‹å­—ã€‚æˆ‘æ©«è±ç¡ä¸è‘—ï¼Œä»”ç´°çœ‹äº†åŠå¤œï¼Œæ‰å¾å­—ç¸«è£¡çœ‹å‡ºå­—ä¾†ï¼Œæ»¿æœ¬éƒ½å¯«è‘—å…©å€‹å­—æ˜¯"åƒäºº"ï¼"""

ai_prob_luxun, factors_luxun = analyze_text_app_fixed(luxun_text)
print("="*70)
print("ğŸ§ª è¿´æ­¸æ¸¬è©¦ - é­¯è¿…ã€Šç‹‚äººæ—¥è¨˜ã€‹")
print("="*70)
print(f"\nğŸ“Š æœ€çµ‚ AI è©•åˆ†: {ai_prob_luxun*100:.2f}%")
print(f"é æœŸ: < 50% (å¤æ–‡ç¶“å…¸)")
print(f"çµæœ: {ai_prob_luxun*100:.2f}% {'âœ…' if ai_prob_luxun < 0.50 else 'âŒ'}")
print()
