#!/usr/bin/env python3
"""æœ€çµ‚å¹³è¡¡ç‰ˆè©•åˆ†æ¸¬è©¦ - è©ç´šæ–‡å­¸æª¢æ¸¬ + æœ€å„ªæ¬Šé‡"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np
import re

# æ¸¬è©¦æ–‡æœ¬
ai_text = """
The development of artificial intelligence has become increasingly significant in recent years. 
The technological advancements have led to numerous applications across various industries. 
The implementation of machine learning algorithms has resulted in improved efficiency and accuracy. 
The analysis of data has become more sophisticated and comprehensive. 
The future of technology appears to be closely linked with artificial intelligence.
"""

human_text = """
ä½ çŸ¥é“å—ï¼Œæˆ‘æœ€è¿‘åœ¨æƒ³ä¸€å€‹å•é¡Œã€‚ç‚ºä»€éº¼æœ‰äº›äººå°±æ˜¯ç‰¹åˆ¥æ“…é•·å¯«æ±è¥¿ï¼Ÿ
å¯èƒ½æ˜¯å› ç‚ºä»–å€‘è®€å¾—å¤šï¼Œæˆ–è€…å°±æ˜¯å¤©ç”Ÿçš„æ‰è¯å§ã€‚ä¸éè©±èªªå›ä¾†ï¼Œ
å¯«å¥½æ±è¥¿çœŸçš„ä¸å®¹æ˜“ã€‚è¦æŠŠæƒ³æ³•æ¸…æ¥šåœ°è¡¨é”å‡ºä¾†ï¼Œé‚„è¦è®“äººæ„Ÿèˆˆè¶£ï¼Œ
é€™éœ€è¦æ™‚é–“å’Œç·´ç¿’ã€‚æˆ‘è¦ºå¾—æœ€é‡è¦çš„æ˜¯è¦æœ‰çœŸå¯¦çš„æƒ³æ³•ï¼Œ
è€Œä¸æ˜¯æ©Ÿæ¢°åœ°æ‹¼æ¹Šè©å½™ã€‚ä½ åŒæ„å—ï¼Ÿ
"""

luxun_text = """
æˆ‘ç¿»é–‹æ­·å²ä¸€æŸ¥ï¼Œé€™æ­·å²æ²’æœ‰å¹´ä»£ï¼Œæ­ªæ­ªæ–œæ–œçš„æ¯è‘‰ä¸Šéƒ½å¯«è‘—"ä»ç¾©é“å¾·"å¹¾å€‹å­—ã€‚
æˆ‘æ©«è±ç¡ä¸è‘—ï¼Œä»”ç´°çœ‹äº†åŠå¤œï¼Œæ‰å¾å­—ç¸«è£¡çœ‹å‡ºå­—ä¾†ï¼Œæ»¿æœ¬éƒ½å¯«è‘—å…©å€‹å­—æ˜¯"åƒäºº"ï¼
æˆ‘é€™å›å¯æ˜¯çœŸçš„è¢«åš‡å£äº†ï¼›è¶•ç·Šåˆä¸Šæ­·å²ï¼›å¿ƒè£¡å»çªç„¶ä¸€é™£å¾ˆå†·çš„æˆ°æ…„ã€‚
"""

def score_text(text):
    """æœ€å„ªå¹³è¡¡è©•åˆ† - æ¬Šé‡çµ„åˆ"""
    ai_score = 0
    score_factors = {}
    
    words = text.split()
    words_lower = [w.lower() for w in words]
    
    # 1. è©å½™å¤šæ¨£æ€§ (28%) - ä¿å®ˆ
    if len(words_lower) > 0:
        unique_words = len(set(words_lower))
        vocab_ratio = unique_words / len(words_lower)
        vocab_score = max(0, min((vocab_ratio - 0.5) / 0.3, 1)) * 0.28
        ai_score += vocab_score
        score_factors['vocabulary_diversity'] = vocab_score
    
    # 2. å¥å­ä¸€è‡´æ€§ (28%) - ä¿å®ˆ
    sentences = [s.strip() for s in 
               text.replace('ã€‚', '.|').replace('ï¼', '!|').replace('ï¼Ÿ', '?|')
                   .replace('.', '.|').replace('!', '!|').replace('?', '?|')
                   .split('|') if s.strip()]
    if len(sentences) > 1:
        sent_lengths = [len(s.split()) for s in sentences]
        mean_len = np.mean(sent_lengths)
        std_len = np.std(sent_lengths)
        cv = std_len / (mean_len + 1e-6) if mean_len > 0 else 0
        consistency_score = max(0, 1 - min(cv, 1.2) / 1.2) * 0.28
        ai_score += consistency_score
        score_factors['sentence_consistency'] = consistency_score
    
    # 3. åŠŸèƒ½è© (8%)
    if any(ord(c) < 128 for c in text):
        function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'of',
                         'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had'}
        if len(words_lower) > 0:
            func_word_count = sum(1 for w in words_lower if w in function_words)
            func_ratio = func_word_count / len(words_lower)
            func_score = min(func_ratio / 0.30, 1) * 0.08
            ai_score += func_score
            score_factors['function_words'] = func_score
    
    # 4. æ¨™é» (8%)
    punct_chars = '.,!?;:\'"â€”-ã€‚ï¼ï¼Ÿï¼›ï¼š''""'
    total_punct = sum(1 for c in text if c in punct_chars)
    punct_density = total_punct / max(len(text), 1)
    
    if punct_density < 0.015:
        punct_score = 0.0
    elif punct_density < 0.03:
        punct_score = 0.04
    else:
        punct_score = 0.08
    
    ai_score += punct_score
    score_factors['punctuation_pattern'] = punct_score
    
    # 5. æ–‡å­¸é¢¨æ ¼æª¢æ¸¬ - æå‡æ¬Šé‡ (20%)
    literary_markers = {
        # ä¸­æ–‡æ–‡å­¸è©å½™
        'ç„¶è€Œ', 'æ—¢ç„¶', 'è«è‹¥', 'å…¶å¯¦', 'æ³ä¸”', 'è€Œæ³', 'ä¸æ–™', 'è±ˆæ–™',
        'æƒ³ä¸åˆ°', 'æ€æ–™', 'èª°çŸ¥', 'å»', 'ç«Ÿ', 'ç«Ÿç„¶', 'åå', 'æ°å¥½',
        'æ°æ°', 'æ­£å¥½', 'æ¹Šå·§', 'æ€ªä¸å¾—', 'ä¹Ÿé›£æ€ª', 'ä¹Ÿæ€ªå¾—',
        'æ‚²å“€', 'æ·’æ¶¼', 'è’¼æ¶¼', 'è’æ¶¼', 'å¯‚å¯¥', 'å­¤å¯‚', 'é ¹å»¢',
        'è¿·èŒ«', 'å›°é “', 'æŠ‘é¬±', 'æ²‰æ‚¶', 'å£“æŠ‘', 'çª’æ¯',
        'å‘¢å–ƒ', 'ä½è²', 'è¼•è²', 'ç´°èª', 'å–ƒå–ƒ', 'å›ˆèª',
        'æ±¨æ±¨', 'æ½ºæ½º', 'æ·™æ·™', 'æ‚ æ‚ ', 'æ‚„æ‚„', 'é»˜é»˜',
        'é£„é£„ç„¶', 'æ¸ºæ¸ºç„¶', 'èŒ«èŒ«ç„¶', 'æ†‘æ·»', 'æ·»å¢', 'è¡ç”Ÿ',
        # é­¯è¿…ç‰¹æœ‰è©å½™
        'æ©«è±', 'ä»”ç´°', 'æˆ°æ…„', 'æ­ªæ­ªæ–œæ–œ', 'åƒäºº', 'å­—ç¸«',
        'ç¿»é–‹', 'æ­·å²', 'ä»ç¾©', 'é“å¾·', 'æ»¿æœ¬',
        # è‹±æ–‡æ–‡å­¸è©å½™
        'alas', 'behold', 'hark', 'lo', 'methinks', 'perchance',
        'forsooth', 'thus', 'verily', 'hence', 'whence', 'thence',
        'woe', 'sorrow', 'anguish', 'melancholy', 'languish',
    }
    
    literary_count = 0
    text_lower = text.lower()
    
    # æª¢æŸ¥ä¸­æ–‡è©
    for marker in literary_markers:
        if '\u4e00' <= marker[0] <= '\u9fff':  # ä¸­æ–‡ç¯„åœ
            if marker in text_lower:
                literary_count += 1
    
    # è‹±æ–‡è©ç”¨è©é‚Šç•Œæª¢æ¸¬
    for marker in literary_markers:
        if ord(marker[0]) < 128:  # è‹±æ–‡
            if re.search(r'\b' + marker + r'\b', text_lower):
                literary_count += 1
    
    if literary_count > 0:
        # æœ‰æ–‡å­¸æ¨™è¨˜ -> å¼·çƒˆæ¸›ä½åˆ†æ•¸
        # å…¬å¼ï¼šliterary_count * 0.20ï¼ˆæ¯å€‹æ¨™è¨˜æ¸› 0.20ï¼‰
        literary_penalty = min(literary_count * 0.20, 0.5)
        ai_score -= literary_penalty
        score_factors['literary_style'] = -literary_penalty
    else:
        score_factors['literary_style'] = 0.0
    
    # 6. çµæ§‹ (8%)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    if len(paragraphs) <= 1:
        ai_score += 0.04
        score_factors['structure'] = 0.04
    else:
        para_lengths = [len(p.split()) for p in paragraphs]
        para_std = np.std(para_lengths)
        para_mean = np.mean(para_lengths)
        para_cv = para_std / (para_mean + 1e-6) if para_mean > 0 else 0
        struct_score = max(0, 1 - min(para_cv, 1)) * 0.08
        ai_score += struct_score
        score_factors['structure'] = struct_score
    
    ai_prob = max(0, min(ai_score, 1.0))
    return ai_prob, score_factors

# é¡è‰²
GREEN = '\033[92m'
RED = '\033[91m'
BLUE = '\033[94m'
YELLOW = '\033[93m'
END = '\033[0m'

print(f"\n{BLUE}{'='*70}")
print("  AI åµæ¸¬ç³»çµ± - æœ€çµ‚å¹³è¡¡ç‰ˆ (æ¬Šé‡: 28-28-8-8-20-8)")
print(f"{'='*70}{END}\n")

test_cases = [
    ("ğŸ¤– AI ç”Ÿæˆæ–‡æœ¬", ai_text, True),
    ("ğŸ‘¤ äººé¡æ–‡æœ¬", human_text, False),
    ("ğŸ“š é­¯è¿…ã€Šç‹‚äººæ—¥è¨˜ã€‹", luxun_text, False),
]

print("æ¸¬è©¦çµæœ:\n")
results = []

for name, text, is_ai in test_cases:
    ai_prob, factors = score_text(text)
    results.append((name, ai_prob, is_ai))
    
    if is_ai and ai_prob > 0.5:
        status = f"{GREEN}âœ…{END}"
    elif not is_ai and ai_prob < 0.5:
        status = f"{GREEN}âœ…{END}"
    else:
        status = f"{YELLOW}âš ï¸{END}"
    
    print(f"{name:20} â†’ {ai_prob:6.2%} AI {status}")
    print(f"   åˆ†æ•¸å› å­:")
    for k, v in factors.items():
        if v != 0:
            print(f"     - {k:25} = {v:+.3f}")
    print()

# é©—è­‰çµæœ
print(f"{BLUE}{'='*70}{END}")
print("é©—è­‰çµæœ:\n")

success = 0
for name, ai_prob, is_ai in results:
    if is_ai and ai_prob > 0.5:
        print(f"{GREEN}âœ…{END} {name:20} â†’ {ai_prob:.2%} AI (æ­£ç¢º)")
        success += 1
    elif not is_ai and ai_prob < 0.5:
        print(f"{GREEN}âœ…{END} {name:20} â†’ {ai_prob:.2%} AI (æ­£ç¢º)")
        success += 1
    else:
        print(f"{RED}âŒ{END} {name:20} â†’ {ai_prob:.2%} AI (èª¤åˆ¤)")

print(f"\næ¸¬è©¦é€šéç‡: {success}/{len(test_cases)} ({100*success//len(test_cases)}%)\n")

if success == len(test_cases):
    print(f"{GREEN}âœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼{END}\n")
else:
    print(f"{YELLOW}âš ï¸  éƒ¨åˆ†æ¸¬è©¦æœªé€šéï¼Œå¾®èª¿ä¸­{END}\n")
