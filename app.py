"""
AI åµæ¸¬ç³»çµ± - Streamlit Web ä»‹é¢ (é›²ç«¯å„ªåŒ–ç‰ˆ)
æ”¯æŒ Streamlit Cloudã€å®Œæ•´çš„ AI åˆ¤åˆ¥é‚è¼¯
åŸºæ–¼ https://justdone.com/ai-detector çš„UI/UXè¨­è¨ˆéˆæ„Ÿ
"""

import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import time
import json

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from utils.feature_extractor import FeatureExtractor
from utils.data_manager import create_dataset, create_json_dataset, load_dataset
from utils.xai_visualizer import XAIVisualizer
from models.ai_detector import AIDetector


# ===== é é¢é…ç½® =====
st.set_page_config(
    page_title="AI Detector",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
)

# è‡ªå®šç¾©æ¨£å¼
st.markdown("""
<style>
    .main {
        padding-top: 1rem;
    }
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
        font-size: 1.1rem;
        font-weight: bold;
    }
    .ai-result {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 15px;
        text-align: center;
        margin: 20px 0;
    }
    .human-result {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
        padding: 30px;
        border-radius: 15px;
        text-align: center;
        margin: 20px 0;
    }
    .result-title {
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 10px;
    }
    .result-subtitle {
        font-size: 1.1rem;
        opacity: 0.9;
    }
    .result-score {
        font-size: 3rem;
        font-weight: bold;
        margin: 15px 0;
    }
</style>
""", unsafe_allow_html=True)


# ===== å´é‚Šæ¬„é…ç½® =====
st.sidebar.title("ğŸ¤– AI Detector")
st.sidebar.markdown("---")

# èªè¨€é¸æ“‡
language = st.sidebar.radio("Language / èªè¨€", ["English", "ä¸­æ–‡"])

# åˆå§‹åŒ– Streamlit session state
if 'detector' not in st.session_state:
    st.session_state.detector = None
if 'feature_extractor' not in st.session_state:
    st.session_state.feature_extractor = FeatureExtractor()
if 'prediction_result' not in st.session_state:
    st.session_state.prediction_result = None
if 'input_text' not in st.session_state:
    st.session_state.input_text = ""


def get_language_strings(lang):
    """æ ¹æ“šèªè¨€è¿”å›ç¿»è­¯å­—ç¬¦ä¸²"""
    strings = {
        'English': {
            'title': 'AI Detector',
            'subtitle': 'Advanced AI-Generated Text Detection System',
            'description': 'Detect AI-generated vs Human-written text using advanced NLP analysis',
            'analyze_title': 'Paste Your Text',
            'analyze_btn': 'Analyze Text',
            'analyzing': 'Analyzing...',
            'results_title': 'Results',
            'ai_detected': 'AI-Generated Content Detected',
            'human_detected': 'Human-Written Content Detected',
            'ai_probability': 'AI Probability',
            'human_probability': 'Human Probability',
            'confidence': 'Confidence Level',
            'features_analyzed': 'Features Analyzed',
            'key_indicators': 'Key Indicators',
            'features_title': 'Text Features',
            'visualization_title': 'Analysis Details',
            'feature_importance': 'Feature Importance',
            'probability_gauge': 'Detection Probability',
            'upload_file': 'Or upload a file',
            'settings_title': 'Model Management',
            'train_title': 'Train New Model',
            'create_dataset_btn': 'Create Dataset',
            'train_model_btn': 'Train Model',
            'training_message': 'Training model...',
            'training_complete': 'Training complete âœ“',
            'error_analyze': 'Error during analysis:',
            'error_model': 'Error loading model:',
        },
        'ä¸­æ–‡': {
            'title': 'AI æ–‡æœ¬åµæ¸¬å™¨',
            'subtitle': 'é«˜éš AI ç”Ÿæˆæ–‡æœ¬åµæ¸¬ç³»çµ±',
            'description': 'ä½¿ç”¨å…ˆé€²çš„ NLP åˆ†ææŠ€è¡“æª¢æ¸¬ AI ç”Ÿæˆå’Œäººé¡æ’°å¯«çš„æ–‡æœ¬',
            'analyze_title': 'ç²˜è²¼æ‚¨çš„æ–‡æœ¬',
            'analyze_btn': 'åˆ†ææ–‡æœ¬',
            'analyzing': 'åˆ†æä¸­...',
            'results_title': 'çµæœ',
            'ai_detected': 'æª¢æ¸¬åˆ° AI ç”Ÿæˆå…§å®¹',
            'human_detected': 'äººé¡æ’°å¯«çš„å…§å®¹',
            'ai_probability': 'AI æ¦‚ç‡',
            'human_probability': 'äººé¡æ¦‚ç‡',
            'confidence': 'ä¿¡å¿ƒåº¦',
            'features_analyzed': 'åˆ†æçš„ç‰¹å¾µ',
            'key_indicators': 'é—œéµæŒ‡æ¨™',
            'features_title': 'æ–‡æœ¬ç‰¹å¾µ',
            'visualization_title': 'åˆ†æè©³æƒ…',
            'feature_importance': 'ç‰¹å¾µé‡è¦æ€§',
            'probability_gauge': 'åµæ¸¬æ¦‚ç‡',
            'upload_file': 'æˆ–ä¸Šå‚³æ–‡ä»¶',
            'settings_title': 'æ¨¡å‹ç®¡ç†',
            'train_title': 'è¨“ç·´æ–°æ¨¡å‹',
            'create_dataset_btn': 'å»ºç«‹æ•¸æ“šé›†',
            'train_model_btn': 'è¨“ç·´æ¨¡å‹',
            'training_message': 'è¨“ç·´ä¸­...',
            'training_complete': 'è¨“ç·´å®Œæˆ âœ“',
            'error_analyze': 'åˆ†æå‡ºéŒ¯ï¼š',
            'error_model': 'æ¨¡å‹è¼‰å…¥éŒ¯èª¤ï¼š',
        }
    }
    return strings.get(lang, strings['English'])


def get_confidence_level(confidence):
    """æ ¹æ“šç½®ä¿¡åº¦è¿”å›ä¿¡å¿ƒç­‰ç´š"""
    if confidence >= 0.85:
        return "very_high"
    elif confidence >= 0.70:
        return "high"
    elif confidence >= 0.55:
        return "medium"
    else:
        return "low"


def get_ai_judgment(ai_prob, confidence):
    """
    å¢å¼·çš„ AI åˆ¤åˆ¥é‚è¼¯
    åŸºæ–¼ AI æ¦‚ç‡å’Œç½®ä¿¡åº¦é€²è¡Œåˆ¤æ–·
    """
    if confidence < 0.5:
        return "INCONCLUSIVE", "åˆ¤æ–·ä¸ç¢ºå®š"
    
    if ai_prob >= 0.75:
        return "LIKELY AI", "å¾ˆå¯èƒ½æ˜¯ AI ç”Ÿæˆ"
    elif ai_prob >= 0.60:
        return "PROBABLY AI", "å¾ˆå¯èƒ½æ˜¯ AI ç”Ÿæˆ"
    elif 0.49 <= ai_prob < 0.60:
        return "MIXED SIGNALS", "ä¿¡è™Ÿæ··åˆ"
    elif ai_prob >= 0.35:
        return "PROBABLY HUMAN", "å¾ˆå¯èƒ½æ˜¯äººé¡æ’°å¯«"
    elif ai_prob <= 0.25:
        return "LIKELY HUMAN", "éå¸¸å¯èƒ½æ˜¯äººé¡æ’°å¯«"
    else:
        return "LIKELY HUMAN", "éå¸¸å¯èƒ½æ˜¯äººé¡æ’°å¯«"


# ç²å–èªè¨€å­—ç¬¦ä¸²
lang_str = get_language_strings(language)


# ===== ä¸»é é¢ =====
st.title(f"ğŸ¤– {lang_str['title']}")
st.markdown(f"#### {lang_str['subtitle']}")
st.markdown(lang_str['description'])
st.markdown("---")


# ===== æ¨™ç±¤é  =====
tab1, tab2, tab3 = st.tabs([
    "ğŸ” Detection",
    "ğŸ“Š Analysis",
    "âš™ï¸ Settings"
])


# ===== æ¨™ç±¤ 1: æ–‡æœ¬åµæ¸¬ =====
with tab1:
    st.header(lang_str['analyze_title'])
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        input_method = st.radio(
            "Select input method",
            ["ğŸ“ Text Box", "ğŸ“ File Upload"],
            horizontal=True,
            key="input_method"
        )
    
    input_text = ""
    
    if input_method == "ğŸ“ Text Box":
        input_text = st.text_area(
            "Enter your text:",
            height=250,
            placeholder="Paste or type the text you want to analyze here...",
            key="text_input",
            label_visibility="collapsed"
        )
    else:
        uploaded_file = st.file_uploader(
            lang_str['upload_file'],
            type=['txt'],
            label_visibility="collapsed"
        )
        if uploaded_file is not None:
            if uploaded_file.type == 'text/plain':
                input_text = uploaded_file.getvalue().decode('utf-8')
            else:
                st.warning("Only .txt files are currently supported.")
    
    # åˆ†ææŒ‰éˆ•
    if st.button(lang_str['analyze_btn'], use_container_width=True, type="primary", key="analyze_btn"):
        if not input_text.strip():
            st.error("âš ï¸ Please enter or upload text to analyze.")
        else:
            try:
                # è¼‰å…¥æˆ–åˆå§‹åŒ–æ¨¡å‹
                model_path = "models/ai_detector_model.pkl"
                
                if st.session_state.detector is None:
                    with st.spinner(lang_str['analyzing']):
                        try:
                            st.session_state.detector = AIDetector(model_path=model_path)
                        except:
                            # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œå‰µå»ºæ–°çš„åµæ¸¬å™¨
                            st.session_state.detector = AIDetector()
                
                # é€²è¡Œé æ¸¬
                with st.spinner(lang_str['analyzing']):
                    time.sleep(0.5)
                    
                    if st.session_state.detector.classifier is not None:
                        prediction = st.session_state.detector.predict(input_text)
                    else:
                        # åªé€²è¡Œç‰¹å¾µåˆ†æ - ä½¿ç”¨æœ€å„ªåŒ–çš„è©•åˆ†é‚è¼¯
                        features = st.session_state.feature_extractor.extract_all_features(input_text)
                        
                        # ===== æœ€å„ªåŒ–çš„ AI åµæ¸¬è©•åˆ†é‚è¼¯ =====
                        ai_score = 0
                        score_factors = {}
                        
                        # æ¸…ç†å’Œæº–å‚™æ–‡æœ¬
                        words = input_text.split()
                        words_lower = [w.lower() for w in words]
                        
                        # 1. è©å½™å¤šæ¨£æ€§ (31% â†‘)
                        if len(words_lower) > 0:
                            unique_words = len(set(words_lower))
                            vocab_ratio = unique_words / len(words_lower)
                            vocab_score = max(0, min((vocab_ratio - 0.54) / 0.26, 1)) * 0.31
                            ai_score += vocab_score
                            score_factors['vocabulary_diversity'] = vocab_score
                        
                        # 2. å¥å­ä¸€è‡´æ€§ (29% â†“)
                        sentences = [s.strip() for s in 
                                   input_text.replace('ã€‚', '.|').replace('ï¼', '!|').replace('ï¼Ÿ', '?|')
                                           .replace('.', '.|').replace('!', '!|').replace('?', '?|')
                                           .split('|') if s.strip()]
                        if len(sentences) > 1:
                            sent_lengths = [len(s.split()) for s in sentences]
                            mean_len = np.mean(sent_lengths)
                            std_len = np.std(sent_lengths)
                            cv = std_len / (mean_len + 1e-6) if mean_len > 0 else 0
                            consistency_score = max(0, 1 - min(cv, 1.3) / 1.3) * 0.29
                            ai_score += consistency_score
                            score_factors['sentence_consistency'] = consistency_score
                        
                        # 3. è‹±æ–‡åŠŸèƒ½è© (8%)
                        if any(ord(c) < 128 for c in input_text):
                            function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'of',
                                            'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had'}
                            if len(words_lower) > 0:
                                func_word_count = sum(1 for w in words_lower if w in function_words)
                                func_ratio = func_word_count / len(words_lower)
                                func_score = min(func_ratio / 0.30, 1) * 0.08
                                ai_score += func_score
                                score_factors['function_words'] = func_score
                        
                        # 4. æ¨™é»ç¬¦è™Ÿå¯†åº¦ (6%)
                        punct_chars = '.,!?;:\'"â€”-ã€‚ï¼ï¼Ÿï¼›ï¼š''""'
                        total_punct = sum(1 for c in input_text if c in punct_chars)
                        punct_density = total_punct / max(len(input_text), 1)
                        
                        if punct_density < 0.015:
                            punct_score = 0.0
                        elif punct_density < 0.03:
                            punct_score = 0.03
                        else:
                            punct_score = 0.06
                        
                        ai_score += punct_score
                        score_factors['punctuation_pattern'] = punct_score
                        
                        # 5. å¤æ–‡/ç¶“å…¸æ–‡å­¸æª¢æ¸¬ (10% â†“)
                        import re
                        
                        # å¤æ–‡/ç¶“å…¸æ–‡å­¸æ¨™è¨˜ (æ‡‰è©²æ‡²ç½°)
                        classical_literary_markers = {
                            # ä¸­æ–‡å¤æ–‡è©å½™
                            'ç„¶è€Œ', 'æ—¢ç„¶', 'è«è‹¥', 'å…¶å¯¦', 'æ³ä¸”', 'è€Œæ³', 'ä¸æ–™', 'è±ˆæ–™',
                            'æƒ³ä¸åˆ°', 'æ€æ–™', 'èª°çŸ¥', 'å»', 'ç«Ÿ', 'ç«Ÿç„¶', 'åå', 'æ°å¥½',
                            'æ°æ°', 'æ­£å¥½', 'æ¹Šå·§', 'æ€ªä¸å¾—', 'ä¹Ÿé›£æ€ª', 'ä¹Ÿæ€ªå¾—',
                            'æ©«è±', 'ä»”ç´°', 'æˆ°æ…„', 'æ­ªæ­ªæ–œæ–œ', 'åƒäºº', 'å­—ç¸«',
                            'ç¿»é–‹', 'æ­·å²', 'ä»ç¾©', 'é“å¾·', 'æ»¿æœ¬',
                            # è‹±æ–‡å¤å…¸è©å½™
                            'alas', 'behold', 'hark', 'lo', 'methinks', 'perchance',
                            'forsooth', 'thus', 'verily', 'hence', 'whence', 'thence',
                            'thee', 'thou', 'thy', 'hath', 'doth', 'wherefore',
                        }
                        
                        classical_count = 0
                        text_lower = input_text.lower()
                        
                        for marker in classical_literary_markers:
                            if '\u4e00' <= marker[0] <= '\u9fff':
                                if marker in text_lower:
                                    classical_count += 1
                            elif ord(marker[0]) < 128:
                                if re.search(r'\b' + marker + r'\b', text_lower):
                                    classical_count += 1
                        
                        if classical_count > 0:
                            classical_penalty = min(classical_count * 0.25, 0.40)
                            ai_score -= classical_penalty
                            score_factors['literary_style'] = -classical_penalty
                        else:
                            score_factors['literary_style'] = 0.0
                        
                        # 5.5 æµªæ¼«/æƒ…æ„Ÿå…§å®¹æª¢æ¸¬ (ä¸æ‡²ç½°,åƒ…ä½œç‚ºæ¨™ç±¤)
                        # æª¢æ¸¬ç¾ä»£æµªæ¼«/æ„Ÿæƒ…è©å½™ - é€™ä¸æ‡‰è©²è¢«è¦–ç‚ºå¤æ–‡æ¨™è¨˜
                        romantic_emotional_words = {
                            # è‹±æ–‡æµªæ¼«è©å½™
                            'love', 'heart', 'smile', 'warmth', 'embrace', 'promise',
                            'fire', 'silence', 'perfect', 'familiar', 'softly', 'closer',
                            'traced', 'whisper', 'blurred', 'watercolors', 'amber', 'glow',
                            'breathing', 'murmured', 'kissing', 'admitted', 'troubled',
                            'borrowed', 'countered', 'foreheads', 'scent', 'clung',
                            'sweater', 'stillness', 'chaos', 'undeniable', 'pensive',
                            'wrapped', 'completely', 'whispered', 'storms', 'waiting',
                            'tightening', 'moon', 'vow', 'fireworks', 'solidity',
                            'surveillance', 'tender', 'gentle', 'passionate', 'desire',
                            'longing', 'yearning', 'adore', 'cherish', 'beloved',
                            # ä¸­æ–‡æµªæ¼«è©å½™
                            'æ„›', 'å¿ƒ', 'æº«æš–', 'æ“æŠ±', 'æ‰¿è«¾', 'ç«', 'æ²‰é»˜', 'å®Œç¾',
                            'ç†Ÿæ‚‰', 'è¼•è¼•', 'é è¿‘', 'æç¹ª', 'ä½èª', 'è¦ªå»', 'æ‰¿èª',
                        }
                        
                        romantic_count = 0
                        words_list = input_text.lower().split()
                        for word in words_list:
                            clean_word = word.strip('.,!?;:\'"')
                            if clean_word in romantic_emotional_words:
                                romantic_count += 1
                        
                        # æª¢æŸ¥ä¸­æ–‡æµªæ¼«è©
                        for marker in romantic_emotional_words:
                            if '\u4e00' <= marker[0] <= '\u9fff':
                                if marker in text_lower:
                                    romantic_count += 1
                        
                        score_factors['romantic_content'] = romantic_count
                        # æ³¨æ„: ä¸æœƒæ¸›å°‘ ai_score,å› ç‚ºæµªæ¼«å…§å®¹å¯ä»¥èˆ‡ AI ç”Ÿæˆä¸¦å­˜
                        
                        # 6. äººæ€§åŒ–æ¨™è¨˜ (7%)
                        humanization_score = 0
                        
                        question_count = input_text.count('?') + input_text.count('ï¼Ÿ')
                        question_ratio = question_count / max(len(sentences), 1)
                        if question_ratio > 0.15:
                            humanization_score += 0.035
                        
                        ellipsis_count = input_text.count('...') + input_text.count('ã€‚ã€‚ã€‚')
                        if ellipsis_count > 0:
                            humanization_score += 0.02
                        
                        personal_words = {'æˆ‘', 'æˆ‘è¦ºå¾—', 'æˆ‘èªç‚º', 'æˆ‘æƒ³', 'æˆ‘ç™¼ç¾', 'æˆ‘çœ‹', 
                                        'i think', 'i feel', 'i believe', 'in my opinion'}
                        personal_count = 0
                        for word in personal_words:
                            if '\u4e00' <= word[0] <= '\u9fff':
                                if word in text_lower:
                                    personal_count += 1
                            else:
                                if re.search(r'\b' + word + r'\b', text_lower):
                                    personal_count += 1
                        
                        if personal_count > 1:
                            humanization_score += min(personal_count * 0.015, 0.015)
                        
                        ai_score -= min(humanization_score, 0.07)
                        score_factors['humanization'] = -min(humanization_score, 0.07)
                        
                        # 7. çµæ§‹è¦å¾‹æ€§ (9% â†‘ to reach 100% total: 31+29+8+6+10+7+9=100%)
                        paragraphs = [p.strip() for p in input_text.split('\n\n') if p.strip()]
                        if len(paragraphs) <= 1:
                            ai_score += 0.045
                            score_factors['structure'] = 0.045
                        else:
                            para_lengths = [len(p.split()) for p in paragraphs]
                            para_std = np.std(para_lengths)
                            para_mean = np.mean(para_lengths)
                            para_cv = para_std / (para_mean + 1e-6) if para_mean > 0 else 0
                            struct_score = max(0, 1 - min(para_cv, 1)) * 0.09
                            ai_score += struct_score
                            score_factors['structure'] = struct_score
                        
                        # ç¢ºä¿åˆ†æ•¸åœ¨ [0, 1] ç¯„åœå…§
                        ai_prob = max(0, min(ai_score, 1.0))
                        
                        # è¨ˆç®—ç½®ä¿¡åº¦
                        confidence = max(abs(ai_prob - 0.5) * 2, 0.5)
                        
                        prediction = {
                            'prediction': 1 if ai_prob >= 0.5 else 0,
                            'ai_probability': ai_prob,
                            'human_probability': 1 - ai_prob,
                            'confidence': confidence,
                            'extracted_features': features,
                            'score_factors': score_factors,
                        }
                
                # å„²å­˜çµæœ
                st.session_state.prediction_result = prediction
                st.session_state.input_text = input_text
                
            except Exception as e:
                st.error(f"{lang_str['error_analyze']} {str(e)}")
    
    # é¡¯ç¤ºçµæœ
    if st.session_state.prediction_result is not None:
        st.markdown("---")
        
        prediction = st.session_state.prediction_result
        ai_prob = prediction['ai_probability']
        human_prob = prediction['human_probability']
        confidence = prediction['confidence']
        
        # ç²å– AI åˆ¤åˆ¥çµæœ
        judgment, judgment_cn = get_ai_judgment(ai_prob, confidence)
        
        # æ ¹æ“šçµæœé¡¯ç¤ºä¸åŒçš„æ¨£å¼
        if ai_prob >= 0.5:
            # AI ç”Ÿæˆ
            st.markdown(f"""
            <div class="ai-result">
                <div class="result-title">ğŸ¤– {judgment}</div>
                <div class="result-subtitle">{lang_str['ai_detected']}</div>
                <div class="result-score">{ai_prob:.1%}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            # äººé¡æ’°å¯«
            st.markdown(f"""
            <div class="human-result">
                <div class="result-title">âœï¸ {judgment}</div>
                <div class="result-subtitle">{lang_str['human_detected']}</div>
                <div class="result-score">{human_prob:.1%}</div>
            </div>
            """, unsafe_allow_html=True)
        
        # è©³ç´°æŒ‡æ¨™
        st.markdown(f"### {lang_str['results_title']}")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ğŸ¤– AI æ¦‚ç‡",
                f"{ai_prob:.1%}"
            )
        
        with col2:
            st.metric(
                "ğŸ‘¤ äººé¡æ¦‚ç‡",
                f"{human_prob:.1%}"
            )
        
        with col3:
            confidence_level = get_confidence_level(confidence)
            confidence_text = {
                'very_high': 'éå¸¸é«˜',
                'high': 'é«˜',
                'medium': 'ä¸­ç­‰',
                'low': 'ä½'
            }.get(confidence_level, 'ä¸­ç­‰')
            st.metric(
                "ğŸ“Š ä¿¡å¿ƒåº¦",
                f"{confidence:.1%}",
                f"{confidence_text}"
            )
        
        with col4:
            word_count = len(st.session_state.input_text.split())
            st.metric(
                "ğŸ“ å­—æ•¸",
                f"{word_count}"
            )


# ===== æ¨™ç±¤ 2: è©³ç´°åˆ†æ =====
with tab2:
    st.header(lang_str['visualization_title'])
    
    if st.session_state.prediction_result is not None:
        prediction = st.session_state.prediction_result
        features = prediction['extracted_features']
        
        # ç‰¹å¾µæ¦‚è¦½
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š Key Features")
            
            # æ§‹å»ºç‰¹å¾µæŒ‡æ¨™
            feature_indicators = []
            
            # Perplexity
            if features.get('pp_avg_perplexity'):
                feature_indicators.append(f"ğŸ”¤ Perplexity: {features['pp_avg_perplexity']:.2f}")
            
            # Burstiness
            if features.get('burst_burstiness'):
                feature_indicators.append(f"ğŸ“ˆ Burstiness: {features['burst_burstiness']:.4f}")
            
            # TTR
            if features.get('style_ttr'):
                feature_indicators.append(f"ğŸ¨ TTR: {features['style_ttr']:.4f}")
            
            # åŠŸèƒ½è©
            if features.get('style_func_word_ratio'):
                feature_indicators.append(f"ğŸ’¬ Function Words: {features['style_func_word_ratio']:.2%}")
            
            for indicator in feature_indicators:
                st.write(f"â€¢ {indicator}")
        
        with col2:
            st.subheader("ğŸ¯ Scoring Breakdown")
            
            if 'score_factors' in prediction:
                score_df = pd.DataFrame([
                    {'Factor': k.replace('_', ' ').title(), 'Score': f"{v:.2%}"}
                    for k, v in prediction['score_factors'].items()
                ])
                st.dataframe(score_df, use_container_width=True, hide_index=True)
        
        # è©³ç´°ç‰¹å¾µè¡¨
        st.markdown("---")
        st.subheader(lang_str['features_title'])
        
        # æŒ‰é¡åˆ¥åˆ†çµ„
        feature_categories = {
            'Perplexity (å›°æƒ‘åº¦)': {k: v for k, v in features.items() if k.startswith('pp_')},
            'Burstiness (çˆ†ç™¼åº¦)': {k: v for k, v in features.items() if k.startswith('burst_')},
            'Stylometry (æ–‡é¢¨)': {k: v for k, v in features.items() if k.startswith('style_')},
            'Zipf Distribution': {k: v for k, v in features.items() if k.startswith('zipf_')},
        }
        
        for category, feats in feature_categories.items():
            if feats:
                with st.expander(f"ğŸ“‹ {category}"):
                    for feat_name, feat_value in feats.items():
                        col1, col2 = st.columns([1, 2])
                        with col1:
                            st.code(feat_name)
                        with col2:
                            st.metric("", f"{feat_value:.6f}" if isinstance(feat_value, float) else str(feat_value))
        
        # åŸå§‹æ–‡æœ¬é è¦½
        st.markdown("---")
        st.subheader("ğŸ“ Analyzed Text Preview")
        st.text_area(
            "Text preview (first 500 characters)",
            value=st.session_state.input_text[:500] + "..." if len(st.session_state.input_text) > 500 else st.session_state.input_text,
            height=150,
            disabled=True
        )
    
    else:
        st.info("ğŸ‘ˆ Please analyze a text first in the Detection tab.")


# ===== æ¨™ç±¤ 3: è¨­ç½® =====
with tab3:
    st.header(lang_str['settings_title'])
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“Š Dataset Management")
        
        if st.button(lang_str['create_dataset_btn'], use_container_width=True, key="create_dataset"):
            with st.spinner(lang_str['training_message']):
                try:
                    Path('data').mkdir(exist_ok=True)
                    create_dataset('data/training_data_en.csv', language='english')
                    create_json_dataset('data/training_data_en.json', language='english')
                    st.success("âœ“ Dataset created successfully!")
                except Exception as e:
                    st.error(f"Error: {str(e)}")
    
    with col2:
        st.subheader(lang_str['train_title'])
        
        if st.button(lang_str['train_model_btn'], use_container_width=True, key="train_model"):
            with st.spinner(lang_str['training_message']):
                try:
                    # ç¢ºä¿æ•¸æ“šé›†å­˜åœ¨
                    dataset_path = 'data/training_data_en.csv'
                    if not Path(dataset_path).exists():
                        create_dataset(dataset_path, language='english')
                    
                    # è¨“ç·´æ¨¡å‹
                    detector = AIDetector()
                    results = detector.train(dataset_path, test_size=0.2)
                    
                    # ä¿å­˜æ¨¡å‹
                    Path('models').mkdir(exist_ok=True)
                    detector.save_model('models/ai_detector_model.pkl')
                    
                    # æ›´æ–° session state
                    st.session_state.detector = detector
                    
                    st.success(lang_str['training_complete'])
                    
                    # é¡¯ç¤ºçµæœ
                    st.subheader("Training Results")
                    cols = st.columns(3)
                    with cols[0]:
                        st.metric("Train Accuracy", f"{results['train_accuracy']:.2%}")
                    with cols[1]:
                        st.metric("Test Accuracy", f"{results['test_accuracy']:.2%}")
                    with cols[2]:
                        st.metric("F1 Score", f"{results['test_f1']:.4f}")
                    
                except Exception as e:
                    st.error(f"{lang_str['error_model']} {str(e)}")
    
    st.markdown("---")
    st.subheader("â„¹ï¸ Model Information")
    
    if st.session_state.detector and st.session_state.detector.classifier:
        st.success("âœ… Model loaded and ready")
        st.info(f"Features: {len(st.session_state.detector.feature_names) if hasattr(st.session_state.detector, 'feature_names') else 'N/A'}")
    else:
        st.warning("âš ï¸ No trained model loaded. AI detection will use heuristic analysis.")
    
    # ä½¿ç”¨èªªæ˜
    st.markdown("---")
    st.subheader("ğŸ“– How to Use")
    
    with st.expander("How does AI Detection work?"):
        st.markdown("""
        This AI Detector uses advanced NLP techniques to identify AI-generated text:
        
        **Key Features Analyzed:**
        - **Perplexity**: Measures text predictability to language models
        - **Burstiness**: Analyzes sentence length variation patterns
        - **Stylometry**: Studies writing style characteristics
        - **Zipf Distribution**: Examines vocabulary patterns
        
        **Detection Process:**
        1. Extract 20+ linguistic features from input text
        2. Apply machine learning classification
        3. Generate confidence score
        4. Display detailed analysis
        """)
    
    with st.expander("What affects accuracy?"):
        st.markdown("""
        - **Text Length**: Longer texts (300+ words) provide better accuracy
        - **Text Type**: Different domains may have different patterns
        - **AI Model Used**: Different AI models produce different outputs
        - **Writing Style**: Formal or structured writing may resemble AI
        """)


# ===== é å°¾ =====
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #999; font-size: 0.8rem; padding: 20px;">
    <p>ğŸ¤– AI Detector v1.0 | Advanced AI-Generated Text Detection</p>
    <p>Powered by Perplexity â€¢ Burstiness â€¢ Stylometry â€¢ Zipf Analysis</p>
</div>
""", unsafe_allow_html=True)
